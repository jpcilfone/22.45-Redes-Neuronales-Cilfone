{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "66aAnzAV_hq6"
      },
      "outputs": [],
      "source": [
        "import os, re, csv, math, codecs, logging\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from io import StringIO\n",
        "import pickle\n",
        "import gdown\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.metrics import F1Score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cargamos los datos (ya separados de forma predeterminada en train y test)\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "class_num = 20"
      ],
      "metadata": {
        "id": "bq2K-KNtLacL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# descargamos los embeddings de palabras de Fasttext para inglés y descomprimimos el archivo.\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "!unzip wiki-news-300d-1M.vec.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8VHcRGiLARi",
        "outputId": "6f0191cb-cd14-47b6-87b3-5a61a7127d8a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-28 21:26:57--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.96, 3.163.189.51, 3.163.189.108, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M   242MB/s    in 2.7s    \n",
            "\n",
            "2024-06-28 21:27:00 (242 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
            "\n",
            "Archive:  wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cargamos los embeddings de palabras\n",
        "print('loading word embeddings...')\n",
        "embeddings_index = {}\n",
        "f = codecs.open('wiki-news-300d-1M.vec', encoding='utf-8')\n",
        "\n",
        "for line in f:\n",
        "    values = line.rstrip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print(f'found {len(embeddings_index)} word vectors')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc_N4ScILTdp",
        "outputId": "bd9223e2-2048-4a3c-ae0c-377c0ee1a760"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading word embeddings...\n",
            "found 999995 word vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instanciamos el tokenizador\n",
        "token = Tokenizer(num_words=30000,\n",
        "                filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "                lower=True,\n",
        "                split=' ',\n",
        "                char_level=False,\n",
        "                oov_token=\"UNK\",\n",
        "                document_count=0)"
      ],
      "metadata": {
        "id": "WBzcDHYXMEEL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fiteamos el tokenizador\n",
        "token.fit_on_texts(newsgroups_train.data)"
      ],
      "metadata": {
        "id": "hn_6uPC5MZEZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtenemos los diccionarios idx2word y word2idx\n",
        "reverse_dictionary = token.index_word\n",
        "dictionary = dict([(value, key) for (key, value) in reverse_dictionary.items()])\n",
        "# CHECK QUE EMPIEZA POR 0"
      ],
      "metadata": {
        "id": "2ygf-uVlL46e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cargamos en una matriz los embeddings de las palabras\n",
        "# presentes en el vocabulario\n",
        "embed_dim=300\n",
        "num_words=len(dictionary)+1\n",
        "embedding_matrix=np.zeros([num_words,embed_dim])\n",
        "for word, idx in dictionary.items():\n",
        "  if idx <= num_words and word in embeddings_index:\n",
        "    embedding_matrix[idx,:]=embeddings_index[word]"
      ],
      "metadata": {
        "id": "UFWmXJRGMeXK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ajCS18eYBwA",
        "outputId": "9bfa6857-9be5-49dd-9413-943e8f1fb1cf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105374, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicamos principal component analisis\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=200)\n",
        "embedding_matrix_reduced = pca.fit_transform(embedding_matrix)"
      ],
      "metadata": {
        "id": "WAtwdW6sv3mK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# se tokenizan los textos\n",
        "train_sequences=token.texts_to_sequences(newsgroups_train.data)\n",
        "test_sequences=token.texts_to_sequences(newsgroups_test.data)"
      ],
      "metadata": {
        "id": "fGYvFbYwNjfB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# En este punto seleccionamos el tamaño de contexto a procesar en la variable `max_len`\n",
        "max_len=100\n",
        "train_sequences=pad_sequences(train_sequences,maxlen=max_len)\n",
        "test_sequences=pad_sequences(test_sequences,maxlen=max_len)"
      ],
      "metadata": {
        "id": "OkV9rt_9NpKk"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.layers import Bidirectional, LSTM, Dense, Embedding, Dropout, BatchNormalization, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GlobalMaxPooling1D, SpatialDropout1D\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "from keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ],
      "metadata": {
        "id": "QqLNwlPsUSJe"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=num_words, output_dim=200, weights=[embedding_matrix_reduced], input_shape=(None,), trainable = True))\n",
        "model.add(SpatialDropout1D(0.5))\n",
        "model.add(Bidirectional(LSTM(200, return_sequences=True)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Bidirectional(LSTM(200, return_sequences=True)))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(512,kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(32,kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(20,kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# Clasificación multiple categórica --> loss = categorical_crossentropy\n",
        "# notar que usamos la versión Sparse para utilizar sólo índices en lugar de OHE\n",
        "model.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
        "# ^ El modelo da mejores resultados con Adam (para sorpresa de nadie)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AVcocHjTcRt",
        "outputId": "98bb103e-5cc2-4fb2-cbee-56c2d083ff85"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, None, 200)         21074800  \n",
            "                                                                 \n",
            " spatial_dropout1d_1 (Spati  (None, None, 200)         0         \n",
            " alDropout1D)                                                    \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirecti  (None, None, 400)         641600    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, None, 400)         0         \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirecti  (None, None, 400)         961600    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Gl  (None, 400)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 400)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 512)               205312    \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 512)               2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 32)                16416     \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 32)                128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 32)                0         \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 20)                660       \n",
            "                                                                 \n",
            " batch_normalization_9 (Bat  (None, 20)                80        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 20)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22902644 (87.37 MB)\n",
            "Trainable params: 22901516 (87.36 MB)\n",
            "Non-trainable params: 1128 (4.41 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping,LearningRateScheduler,ModelCheckpoint,ReduceLROnPlateau, TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Model Checkpoint\n",
        "mc = ModelCheckpoint(\n",
        "    \"bestweights.h5\",\n",
        "    monitor = \"val_accuracy\",\n",
        "    verbose = 1,\n",
        "    save_best_only = True,\n",
        "    save_weights_only = True,\n",
        ")\n",
        "\n",
        "# Reduce Learning Rate on Plateau\n",
        "rlrop = ReduceLROnPlateau(\n",
        "    monitor = \"val_accuracy\",\n",
        "    factor = 0.5,\n",
        "    patience = 3,\n",
        "    verbose = 1,\n",
        "    min_lr = 1e-5\n",
        ")\n",
        "\n",
        "# Early Stopping\n",
        "es = EarlyStopping(\n",
        "    monitor = \"val_accuracy\",\n",
        "    patience = 10,\n",
        "    verbose = 1,\n",
        "    restore_best_weights = True,\n",
        ")\n",
        "\n",
        "# Tensorboard\n",
        "tb = TensorBoard(\n",
        "    log_dir=\"logs\",\n",
        ")"
      ],
      "metadata": {
        "id": "dXHBMEsKz28B"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_sequences, newsgroups_train.target,\n",
        "                    batch_size=256,\n",
        "                    epochs=100,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[mc,es,rlrop,tb],\n",
        "                    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkWOLsrOVAiz",
        "outputId": "bade14db-4a35-43ff-c253-8bdfc813bd2f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5372 - accuracy: 0.9269\n",
            "Epoch 1: val_accuracy improved from -inf to 0.66549, saving model to bestweights.h5\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.5372 - accuracy: 0.9269 - val_loss: 1.3550 - val_accuracy: 0.6655 - lr: 6.2500e-05\n",
            "Epoch 2/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5335 - accuracy: 0.9266\n",
            "Epoch 2: val_accuracy improved from 0.66549 to 0.66681, saving model to bestweights.h5\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.5335 - accuracy: 0.9266 - val_loss: 1.3564 - val_accuracy: 0.6668 - lr: 6.2500e-05\n",
            "Epoch 3/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5424 - accuracy: 0.9232\n",
            "Epoch 3: val_accuracy improved from 0.66681 to 0.66858, saving model to bestweights.h5\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.5424 - accuracy: 0.9232 - val_loss: 1.3568 - val_accuracy: 0.6686 - lr: 6.2500e-05\n",
            "Epoch 4/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5389 - accuracy: 0.9252\n",
            "Epoch 4: val_accuracy did not improve from 0.66858\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.5389 - accuracy: 0.9252 - val_loss: 1.3625 - val_accuracy: 0.6637 - lr: 6.2500e-05\n",
            "Epoch 5/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5290 - accuracy: 0.9280\n",
            "Epoch 5: val_accuracy did not improve from 0.66858\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.5290 - accuracy: 0.9280 - val_loss: 1.3731 - val_accuracy: 0.6673 - lr: 6.2500e-05\n",
            "Epoch 6/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5243 - accuracy: 0.9302\n",
            "Epoch 6: val_accuracy did not improve from 0.66858\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.5243 - accuracy: 0.9302 - val_loss: 1.3623 - val_accuracy: 0.6677 - lr: 6.2500e-05\n",
            "Epoch 7/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5274 - accuracy: 0.9273\n",
            "Epoch 7: val_accuracy did not improve from 0.66858\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.5274 - accuracy: 0.9273 - val_loss: 1.3635 - val_accuracy: 0.6681 - lr: 3.1250e-05\n",
            "Epoch 8/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5171 - accuracy: 0.9321\n",
            "Epoch 8: val_accuracy did not improve from 0.66858\n",
            "36/36 [==============================] - 5s 147ms/step - loss: 0.5171 - accuracy: 0.9321 - val_loss: 1.3612 - val_accuracy: 0.6686 - lr: 3.1250e-05\n",
            "Epoch 9/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5188 - accuracy: 0.9309\n",
            "Epoch 9: val_accuracy improved from 0.66858 to 0.67079, saving model to bestweights.h5\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 0.5188 - accuracy: 0.9309 - val_loss: 1.3609 - val_accuracy: 0.6708 - lr: 3.1250e-05\n",
            "Epoch 10/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5152 - accuracy: 0.9304\n",
            "Epoch 10: val_accuracy did not improve from 0.67079\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.5152 - accuracy: 0.9304 - val_loss: 1.3598 - val_accuracy: 0.6703 - lr: 3.1250e-05\n",
            "Epoch 11/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5149 - accuracy: 0.9300\n",
            "Epoch 11: val_accuracy did not improve from 0.67079\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.5149 - accuracy: 0.9300 - val_loss: 1.3595 - val_accuracy: 0.6695 - lr: 3.1250e-05\n",
            "Epoch 12/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5056 - accuracy: 0.9351\n",
            "Epoch 12: val_accuracy did not improve from 0.67079\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "36/36 [==============================] - 5s 142ms/step - loss: 0.5056 - accuracy: 0.9351 - val_loss: 1.3694 - val_accuracy: 0.6668 - lr: 3.1250e-05\n",
            "Epoch 13/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5089 - accuracy: 0.9297\n",
            "Epoch 13: val_accuracy did not improve from 0.67079\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.5089 - accuracy: 0.9297 - val_loss: 1.3647 - val_accuracy: 0.6686 - lr: 1.5625e-05\n",
            "Epoch 14/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5081 - accuracy: 0.9305\n",
            "Epoch 14: val_accuracy did not improve from 0.67079\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.5081 - accuracy: 0.9305 - val_loss: 1.3675 - val_accuracy: 0.6686 - lr: 1.5625e-05\n",
            "Epoch 15/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5051 - accuracy: 0.9311\n",
            "Epoch 15: val_accuracy did not improve from 0.67079\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.5051 - accuracy: 0.9311 - val_loss: 1.3640 - val_accuracy: 0.6681 - lr: 1.5625e-05\n",
            "Epoch 16/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5056 - accuracy: 0.9345\n",
            "Epoch 16: val_accuracy improved from 0.67079 to 0.67167, saving model to bestweights.h5\n",
            "36/36 [==============================] - 6s 167ms/step - loss: 0.5056 - accuracy: 0.9345 - val_loss: 1.3636 - val_accuracy: 0.6717 - lr: 1.0000e-05\n",
            "Epoch 17/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5016 - accuracy: 0.9346\n",
            "Epoch 17: val_accuracy did not improve from 0.67167\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.5016 - accuracy: 0.9346 - val_loss: 1.3641 - val_accuracy: 0.6690 - lr: 1.0000e-05\n",
            "Epoch 18/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5043 - accuracy: 0.9308\n",
            "Epoch 18: val_accuracy did not improve from 0.67167\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.5043 - accuracy: 0.9308 - val_loss: 1.3664 - val_accuracy: 0.6686 - lr: 1.0000e-05\n",
            "Epoch 19/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4974 - accuracy: 0.9356\n",
            "Epoch 19: val_accuracy did not improve from 0.67167\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.4974 - accuracy: 0.9356 - val_loss: 1.3665 - val_accuracy: 0.6699 - lr: 1.0000e-05\n",
            "Epoch 20/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4959 - accuracy: 0.9336\n",
            "Epoch 20: val_accuracy did not improve from 0.67167\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.4959 - accuracy: 0.9336 - val_loss: 1.3666 - val_accuracy: 0.6664 - lr: 1.0000e-05\n",
            "Epoch 21/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5016 - accuracy: 0.9353\n",
            "Epoch 21: val_accuracy did not improve from 0.67167\n",
            "36/36 [==============================] - 5s 144ms/step - loss: 0.5016 - accuracy: 0.9353 - val_loss: 1.3660 - val_accuracy: 0.6703 - lr: 1.0000e-05\n",
            "Epoch 22/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4976 - accuracy: 0.9342\n",
            "Epoch 22: val_accuracy improved from 0.67167 to 0.67212, saving model to bestweights.h5\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.4976 - accuracy: 0.9342 - val_loss: 1.3640 - val_accuracy: 0.6721 - lr: 1.0000e-05\n",
            "Epoch 23/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5018 - accuracy: 0.9329\n",
            "Epoch 23: val_accuracy did not improve from 0.67212\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.5018 - accuracy: 0.9329 - val_loss: 1.3641 - val_accuracy: 0.6721 - lr: 1.0000e-05\n",
            "Epoch 24/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4993 - accuracy: 0.9343\n",
            "Epoch 24: val_accuracy improved from 0.67212 to 0.67300, saving model to bestweights.h5\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.4993 - accuracy: 0.9343 - val_loss: 1.3644 - val_accuracy: 0.6730 - lr: 1.0000e-05\n",
            "Epoch 25/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4998 - accuracy: 0.9360\n",
            "Epoch 25: val_accuracy did not improve from 0.67300\n",
            "36/36 [==============================] - 5s 142ms/step - loss: 0.4998 - accuracy: 0.9360 - val_loss: 1.3639 - val_accuracy: 0.6717 - lr: 1.0000e-05\n",
            "Epoch 26/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4968 - accuracy: 0.9338\n",
            "Epoch 26: val_accuracy did not improve from 0.67300\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.4968 - accuracy: 0.9338 - val_loss: 1.3651 - val_accuracy: 0.6712 - lr: 1.0000e-05\n",
            "Epoch 27/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4995 - accuracy: 0.9355\n",
            "Epoch 27: val_accuracy did not improve from 0.67300\n",
            "36/36 [==============================] - 5s 145ms/step - loss: 0.4995 - accuracy: 0.9355 - val_loss: 1.3674 - val_accuracy: 0.6726 - lr: 1.0000e-05\n",
            "Epoch 28/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4994 - accuracy: 0.9312\n",
            "Epoch 28: val_accuracy did not improve from 0.67300\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.4994 - accuracy: 0.9312 - val_loss: 1.3685 - val_accuracy: 0.6703 - lr: 1.0000e-05\n",
            "Epoch 29/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4997 - accuracy: 0.9326\n",
            "Epoch 29: val_accuracy did not improve from 0.67300\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.4997 - accuracy: 0.9326 - val_loss: 1.3676 - val_accuracy: 0.6726 - lr: 1.0000e-05\n",
            "Epoch 30/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5025 - accuracy: 0.9324\n",
            "Epoch 30: val_accuracy improved from 0.67300 to 0.67388, saving model to bestweights.h5\n",
            "36/36 [==============================] - 5s 142ms/step - loss: 0.5025 - accuracy: 0.9324 - val_loss: 1.3676 - val_accuracy: 0.6739 - lr: 1.0000e-05\n",
            "Epoch 31/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4973 - accuracy: 0.9339\n",
            "Epoch 31: val_accuracy did not improve from 0.67388\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.4973 - accuracy: 0.9339 - val_loss: 1.3695 - val_accuracy: 0.6739 - lr: 1.0000e-05\n",
            "Epoch 32/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4957 - accuracy: 0.9335\n",
            "Epoch 32: val_accuracy did not improve from 0.67388\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.4957 - accuracy: 0.9335 - val_loss: 1.3729 - val_accuracy: 0.6721 - lr: 1.0000e-05\n",
            "Epoch 33/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4904 - accuracy: 0.9355\n",
            "Epoch 33: val_accuracy did not improve from 0.67388\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.4904 - accuracy: 0.9355 - val_loss: 1.3716 - val_accuracy: 0.6721 - lr: 1.0000e-05\n",
            "Epoch 34/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4980 - accuracy: 0.9349\n",
            "Epoch 34: val_accuracy did not improve from 0.67388\n",
            "36/36 [==============================] - 5s 147ms/step - loss: 0.4980 - accuracy: 0.9349 - val_loss: 1.3702 - val_accuracy: 0.6703 - lr: 1.0000e-05\n",
            "Epoch 35/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4938 - accuracy: 0.9345\n",
            "Epoch 35: val_accuracy improved from 0.67388 to 0.67477, saving model to bestweights.h5\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.4938 - accuracy: 0.9345 - val_loss: 1.3693 - val_accuracy: 0.6748 - lr: 1.0000e-05\n",
            "Epoch 36/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4939 - accuracy: 0.9333\n",
            "Epoch 36: val_accuracy did not improve from 0.67477\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.4939 - accuracy: 0.9333 - val_loss: 1.3690 - val_accuracy: 0.6739 - lr: 1.0000e-05\n",
            "Epoch 37/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4927 - accuracy: 0.9365\n",
            "Epoch 37: val_accuracy did not improve from 0.67477\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.4927 - accuracy: 0.9365 - val_loss: 1.3708 - val_accuracy: 0.6708 - lr: 1.0000e-05\n",
            "Epoch 38/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4889 - accuracy: 0.9360\n",
            "Epoch 38: val_accuracy did not improve from 0.67477\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.4889 - accuracy: 0.9360 - val_loss: 1.3714 - val_accuracy: 0.6708 - lr: 1.0000e-05\n",
            "Epoch 39/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4845 - accuracy: 0.9364\n",
            "Epoch 39: val_accuracy did not improve from 0.67477\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.4845 - accuracy: 0.9364 - val_loss: 1.3705 - val_accuracy: 0.6734 - lr: 1.0000e-05\n",
            "Epoch 40/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4930 - accuracy: 0.9346\n",
            "Epoch 40: val_accuracy did not improve from 0.67477\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.4930 - accuracy: 0.9346 - val_loss: 1.3740 - val_accuracy: 0.6730 - lr: 1.0000e-05\n",
            "Epoch 41/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4974 - accuracy: 0.9332\n",
            "Epoch 41: val_accuracy did not improve from 0.67477\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.4974 - accuracy: 0.9332 - val_loss: 1.3757 - val_accuracy: 0.6721 - lr: 1.0000e-05\n",
            "Epoch 42/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4943 - accuracy: 0.9328\n",
            "Epoch 42: val_accuracy did not improve from 0.67477\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.4943 - accuracy: 0.9328 - val_loss: 1.3775 - val_accuracy: 0.6708 - lr: 1.0000e-05\n",
            "Epoch 43/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4926 - accuracy: 0.9363\n",
            "Epoch 43: val_accuracy did not improve from 0.67477\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.4926 - accuracy: 0.9363 - val_loss: 1.3775 - val_accuracy: 0.6690 - lr: 1.0000e-05\n",
            "Epoch 44/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4883 - accuracy: 0.9353\n",
            "Epoch 44: val_accuracy did not improve from 0.67477\n",
            "36/36 [==============================] - 5s 147ms/step - loss: 0.4883 - accuracy: 0.9353 - val_loss: 1.3772 - val_accuracy: 0.6681 - lr: 1.0000e-05\n",
            "Epoch 45/100\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.4898 - accuracy: 0.9336\n",
            "Epoch 45: val_accuracy did not improve from 0.67477\n",
            "Restoring model weights from the end of the best epoch: 35.\n",
            "36/36 [==============================] - 5s 147ms/step - loss: 0.4898 - accuracy: 0.9336 - val_loss: 1.3807 - val_accuracy: 0.6690 - lr: 1.0000e-05\n",
            "Epoch 45: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Medir F1-score y accuracy en test\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "y_pred = np.argmax(model.predict(test_sequences), axis=-1)\n",
        "\n",
        "f1 = f1_score(newsgroups_test.target, y_pred, average='macro')\n",
        "accuracy = accuracy_score(newsgroups_test.target, y_pred)\n",
        "\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vKu1HXevgCa",
        "outputId": "f042b3ef-a2c1-4955-8b74-aae439f3c98e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "236/236 [==============================] - 4s 10ms/step\n",
            "F1 Score: 0.6124805438308195\n",
            "Accuracy: 0.6196229421136484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Tokenizacion: opciones DONE\n",
        "Elman, LSTM, GRU\n",
        "Bidireccional     DONE\n",
        "Tamaño de capas y cantidad    DONE\n",
        "Dropout\n",
        "RMSProp, ADAM     DONE\n",
        "BATCH_SIZE DONE\n",
        "Unloop\n",
        "TPU?\n",
        "Embedding entrenable    DONE\n",
        "Forma de colapsar las secuencias\n",
        "Reduccion de dimensionalidad embedding    PCA/DONE\n",
        "'''"
      ],
      "metadata": {
        "id": "6usD8n3bYd36"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}